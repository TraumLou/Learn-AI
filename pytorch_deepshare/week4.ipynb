{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.权值初始化\n",
    "\n",
    "Q:梯度爆炸的代码演示？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:15.959932327270508\n",
      "layer:1, std:256.6237487792969\n",
      "layer:2, std:4107.24560546875\n",
      "layer:3, std:65576.8125\n",
      "layer:4, std:1045011.875\n",
      "layer:5, std:17110408.0\n",
      "layer:6, std:275461408.0\n",
      "layer:7, std:4402537984.0\n",
      "layer:8, std:71323615232.0\n",
      "layer:9, std:1148104736768.0\n",
      "layer:10, std:17911758454784.0\n",
      "layer:11, std:283574846619648.0\n",
      "layer:12, std:4480599809064960.0\n",
      "layer:13, std:7.196814275405414e+16\n",
      "layer:14, std:1.1507761512626258e+18\n",
      "layer:15, std:1.853110740188555e+19\n",
      "layer:16, std:2.9677725826641455e+20\n",
      "layer:17, std:4.780376223769898e+21\n",
      "layer:18, std:7.613223480799065e+22\n",
      "layer:19, std:1.2092652108825478e+24\n",
      "layer:20, std:1.923257075956356e+25\n",
      "layer:21, std:3.134467063655912e+26\n",
      "layer:22, std:5.014437766285408e+27\n",
      "layer:23, std:8.066615144249704e+28\n",
      "layer:24, std:1.2392661553516338e+30\n",
      "layer:25, std:1.9455688099759845e+31\n",
      "layer:26, std:3.0238180658999113e+32\n",
      "layer:27, std:4.950357571077011e+33\n",
      "layer:28, std:8.150925520353362e+34\n",
      "layer:29, std:1.322983152787379e+36\n",
      "layer:30, std:2.0786820453988485e+37\n",
      "layer:31, std:nan\n",
      "output is nan in 31 layers\n",
      "tensor([[        inf, -2.6817e+38,         inf,  ...,         inf,\n",
      "                 inf,         inf],\n",
      "        [       -inf,        -inf,  1.4387e+38,  ..., -1.3409e+38,\n",
      "         -1.9659e+38,        -inf],\n",
      "        [-1.5873e+37,         inf,        -inf,  ...,         inf,\n",
      "                -inf,  1.1484e+38],\n",
      "        ...,\n",
      "        [ 2.7754e+38, -1.6783e+38, -1.5531e+38,  ...,         inf,\n",
      "         -9.9440e+37, -2.5132e+38],\n",
      "        [-7.7184e+37,        -inf,         inf,  ..., -2.6505e+38,\n",
      "                 inf,         inf],\n",
      "        [        inf,         inf,        -inf,  ...,        -inf,\n",
      "                 inf,  1.7432e+38]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from utils.common_tools import set_seed\n",
    "\n",
    "set_seed(1)  # 设置随机种子\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data)    # normal: mean=0, std=1\n",
    "\n",
    "# 100层神经网络\n",
    "layer_nums = 100\n",
    "# 每一层神经元数为256\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:更改初始权值解决不带激活函数全连接网络的梯度爆炸问题？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.9974957704544067\n",
      "layer:1, std:1.0024365186691284\n",
      "layer:2, std:1.002745509147644\n",
      "layer:3, std:1.0006227493286133\n",
      "layer:4, std:0.9966009855270386\n",
      "layer:5, std:1.019859790802002\n",
      "layer:6, std:1.026173710823059\n",
      "layer:7, std:1.0250457525253296\n",
      "layer:8, std:1.0378952026367188\n",
      "layer:9, std:1.0441951751708984\n",
      "layer:10, std:1.0181655883789062\n",
      "layer:11, std:1.0074602365493774\n",
      "layer:12, std:0.9948930144309998\n",
      "layer:13, std:0.9987586140632629\n",
      "layer:14, std:0.9981392025947571\n",
      "layer:15, std:1.0045733451843262\n",
      "layer:16, std:1.0055204629898071\n",
      "layer:17, std:1.0122840404510498\n",
      "layer:18, std:1.0076017379760742\n",
      "layer:19, std:1.000280737876892\n",
      "layer:20, std:0.9943006038665771\n",
      "layer:21, std:1.012800931930542\n",
      "layer:22, std:1.012657642364502\n",
      "layer:23, std:1.018149971961975\n",
      "layer:24, std:0.9776086211204529\n",
      "layer:25, std:0.9592394828796387\n",
      "layer:26, std:0.9317858815193176\n",
      "layer:27, std:0.9534041881561279\n",
      "layer:28, std:0.9811319708824158\n",
      "layer:29, std:0.9953019022941589\n",
      "layer:30, std:0.9773916006088257\n",
      "layer:31, std:0.9655940532684326\n",
      "layer:32, std:0.9270440936088562\n",
      "layer:33, std:0.9329946637153625\n",
      "layer:34, std:0.9311841726303101\n",
      "layer:35, std:0.9354336261749268\n",
      "layer:36, std:0.9492132067680359\n",
      "layer:37, std:0.9679954648017883\n",
      "layer:38, std:0.9849981665611267\n",
      "layer:39, std:0.9982335567474365\n",
      "layer:40, std:0.9616852402687073\n",
      "layer:41, std:0.9439758658409119\n",
      "layer:42, std:0.9631161093711853\n",
      "layer:43, std:0.958673894405365\n",
      "layer:44, std:0.9675614237785339\n",
      "layer:45, std:0.9837557077407837\n",
      "layer:46, std:0.9867278337478638\n",
      "layer:47, std:0.9920817017555237\n",
      "layer:48, std:0.9650403261184692\n",
      "layer:49, std:0.9991624355316162\n",
      "layer:50, std:0.9946174025535583\n",
      "layer:51, std:0.9662044048309326\n",
      "layer:52, std:0.9827387928962708\n",
      "layer:53, std:0.9887880086898804\n",
      "layer:54, std:0.9932605624198914\n",
      "layer:55, std:1.0237400531768799\n",
      "layer:56, std:0.9702046513557434\n",
      "layer:57, std:1.0045380592346191\n",
      "layer:58, std:0.9943899512290955\n",
      "layer:59, std:0.9900636076927185\n",
      "layer:60, std:0.99446702003479\n",
      "layer:61, std:0.9768352508544922\n",
      "layer:62, std:0.9797843098640442\n",
      "layer:63, std:0.9951220750808716\n",
      "layer:64, std:0.9980446696281433\n",
      "layer:65, std:1.0086933374404907\n",
      "layer:66, std:1.0276142358779907\n",
      "layer:67, std:1.0429234504699707\n",
      "layer:68, std:1.0197855234146118\n",
      "layer:69, std:1.0319130420684814\n",
      "layer:70, std:1.0540012121200562\n",
      "layer:71, std:1.026781439781189\n",
      "layer:72, std:1.0331352949142456\n",
      "layer:73, std:1.0666675567626953\n",
      "layer:74, std:1.0413838624954224\n",
      "layer:75, std:1.0733673572540283\n",
      "layer:76, std:1.0404183864593506\n",
      "layer:77, std:1.0344083309173584\n",
      "layer:78, std:1.0022705793380737\n",
      "layer:79, std:0.99835205078125\n",
      "layer:80, std:0.9732587337493896\n",
      "layer:81, std:0.9777462482452393\n",
      "layer:82, std:0.9753198623657227\n",
      "layer:83, std:0.9938382506370544\n",
      "layer:84, std:0.9472599029541016\n",
      "layer:85, std:0.9511011242866516\n",
      "layer:86, std:0.9737769961357117\n",
      "layer:87, std:1.005651831626892\n",
      "layer:88, std:1.0043526887893677\n",
      "layer:89, std:0.9889539480209351\n",
      "layer:90, std:1.0130352973937988\n",
      "layer:91, std:1.0030947923660278\n",
      "layer:92, std:0.9993206262588501\n",
      "layer:93, std:1.0342745780944824\n",
      "layer:94, std:1.031973123550415\n",
      "layer:95, std:1.0413124561309814\n",
      "layer:96, std:1.0817031860351562\n",
      "layer:97, std:1.128799557685852\n",
      "layer:98, std:1.1617802381515503\n",
      "layer:99, std:1.2215303182601929\n",
      "tensor([[-1.0696, -1.1373,  0.5047,  ..., -0.4766,  1.5904, -0.1076],\n",
      "        [ 0.4572,  1.6211,  1.9659,  ..., -0.3558, -1.1235,  0.0979],\n",
      "        [ 0.3908, -0.9998, -0.8680,  ..., -2.4161,  0.5035,  0.2814],\n",
      "        ...,\n",
      "        [ 0.1876,  0.7971, -0.5918,  ...,  0.5395, -0.8932,  0.1211],\n",
      "        [-0.0102, -1.5027, -2.6860,  ...,  0.6954, -0.1858, -0.8027],\n",
      "        [-0.5871, -1.3739, -2.9027,  ...,  1.6734,  0.5094, -0.9986]],\n",
      "       grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from utils.common_tools import set_seed\n",
    "\n",
    "set_seed(1)  # 设置随机种子\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # 注意此处的std\n",
    "                nn.init.normal_(m.weight.data, std=np.sqrt(1/self.neural_num))    # normal: mean=0, std=1\n",
    "\n",
    "# 100层神经网络\n",
    "layer_nums = 100\n",
    "# 每一层神经元数为256\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:添加tanh激活函数后的全连接网络，梯度消失代码示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.6273701786994934\n",
      "layer:1, std:0.48910173773765564\n",
      "layer:2, std:0.4099564850330353\n",
      "layer:3, std:0.35637012124061584\n",
      "layer:4, std:0.32117360830307007\n",
      "layer:5, std:0.2981105148792267\n",
      "layer:6, std:0.27730831503868103\n",
      "layer:7, std:0.2589356303215027\n",
      "layer:8, std:0.2468511462211609\n",
      "layer:9, std:0.23721906542778015\n",
      "layer:10, std:0.22171513736248016\n",
      "layer:11, std:0.21079954504966736\n",
      "layer:12, std:0.19820132851600647\n",
      "layer:13, std:0.19069305062294006\n",
      "layer:14, std:0.18555502593517303\n",
      "layer:15, std:0.17953835427761078\n",
      "layer:16, std:0.17485806345939636\n",
      "layer:17, std:0.1702701896429062\n",
      "layer:18, std:0.16508983075618744\n",
      "layer:19, std:0.1591130942106247\n",
      "layer:20, std:0.15480300784111023\n",
      "layer:21, std:0.15263864398002625\n",
      "layer:22, std:0.148549422621727\n",
      "layer:23, std:0.14617665112018585\n",
      "layer:24, std:0.13876432180404663\n",
      "layer:25, std:0.13316625356674194\n",
      "layer:26, std:0.12660598754882812\n",
      "layer:27, std:0.12537942826747894\n",
      "layer:28, std:0.12535445392131805\n",
      "layer:29, std:0.12589804828166962\n",
      "layer:30, std:0.11994210630655289\n",
      "layer:31, std:0.11700887233018875\n",
      "layer:32, std:0.11137297749519348\n",
      "layer:33, std:0.11154612898826599\n",
      "layer:34, std:0.10991233587265015\n",
      "layer:35, std:0.10996390879154205\n",
      "layer:36, std:0.10969001054763794\n",
      "layer:37, std:0.10975216329097748\n",
      "layer:38, std:0.11063200235366821\n",
      "layer:39, std:0.11021336913108826\n",
      "layer:40, std:0.10465587675571442\n",
      "layer:41, std:0.10141163319349289\n",
      "layer:42, std:0.1026025265455246\n",
      "layer:43, std:0.10079070925712585\n",
      "layer:44, std:0.10096712410449982\n",
      "layer:45, std:0.10117629915475845\n",
      "layer:46, std:0.10145658999681473\n",
      "layer:47, std:0.09987485408782959\n",
      "layer:48, std:0.09677786380052567\n",
      "layer:49, std:0.099615179002285\n",
      "layer:50, std:0.09867013245820999\n",
      "layer:51, std:0.09398546814918518\n",
      "layer:52, std:0.09388342499732971\n",
      "layer:53, std:0.09352942556142807\n",
      "layer:54, std:0.09336657077074051\n",
      "layer:55, std:0.0948176234960556\n",
      "layer:56, std:0.08856320381164551\n",
      "layer:57, std:0.09024856984615326\n",
      "layer:58, std:0.088644839823246\n",
      "layer:59, std:0.08766943216323853\n",
      "layer:60, std:0.08726289123296738\n",
      "layer:61, std:0.08623495697975159\n",
      "layer:62, std:0.08549778908491135\n",
      "layer:63, std:0.0855521708726883\n",
      "layer:64, std:0.0853666365146637\n",
      "layer:65, std:0.08462794870138168\n",
      "layer:66, std:0.0852193832397461\n",
      "layer:67, std:0.08562126755714417\n",
      "layer:68, std:0.08368431031703949\n",
      "layer:69, std:0.08476374298334122\n",
      "layer:70, std:0.0853630006313324\n",
      "layer:71, std:0.08237560093402863\n",
      "layer:72, std:0.08133518695831299\n",
      "layer:73, std:0.08416958898305893\n",
      "layer:74, std:0.08226992189884186\n",
      "layer:75, std:0.08379074186086655\n",
      "layer:76, std:0.08003697544336319\n",
      "layer:77, std:0.07888862490653992\n",
      "layer:78, std:0.07618380337953568\n",
      "layer:79, std:0.07458437979221344\n",
      "layer:80, std:0.07207276672124863\n",
      "layer:81, std:0.07079190015792847\n",
      "layer:82, std:0.0712786465883255\n",
      "layer:83, std:0.07165777683258057\n",
      "layer:84, std:0.06893909722566605\n",
      "layer:85, std:0.0690247192978859\n",
      "layer:86, std:0.07030878216028214\n",
      "layer:87, std:0.07283661514520645\n",
      "layer:88, std:0.07280214875936508\n",
      "layer:89, std:0.07130246609449387\n",
      "layer:90, std:0.07225215435028076\n",
      "layer:91, std:0.0712454542517662\n",
      "layer:92, std:0.07088854163885117\n",
      "layer:93, std:0.0730612576007843\n",
      "layer:94, std:0.07276967912912369\n",
      "layer:95, std:0.07259567081928253\n",
      "layer:96, std:0.07586522400379181\n",
      "layer:97, std:0.07769150286912918\n",
      "layer:98, std:0.07842090725898743\n",
      "layer:99, std:0.08206238597631454\n",
      "tensor([[-0.1103, -0.0739,  0.1278,  ..., -0.0508,  0.1544, -0.0107],\n",
      "        [ 0.0807,  0.1208,  0.0030,  ..., -0.0385, -0.1887, -0.0294],\n",
      "        [ 0.0321, -0.0833, -0.1482,  ..., -0.1133,  0.0206,  0.0155],\n",
      "        ...,\n",
      "        [ 0.0108,  0.0560, -0.1099,  ...,  0.0459, -0.0961, -0.0124],\n",
      "        [ 0.0398, -0.0874, -0.2312,  ...,  0.0294, -0.0562, -0.0556],\n",
      "        [-0.0234, -0.0297, -0.1155,  ...,  0.1143,  0.0083, -0.0675]],\n",
      "       grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from utils.common_tools import set_seed\n",
    "\n",
    "set_seed(1)  # 设置随机种子\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            # 多添加了tanh激活函数\n",
    "            x = torch.tanh(x)\n",
    "\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # 注意此处的std\n",
    "                nn.init.normal_(m.weight.data, std=np.sqrt(1/self.neural_num))    # normal: mean=0, std=1\n",
    "\n",
    "                # a = np.sqrt(6 / (self.neural_num + self.neural_num))\n",
    "                #\n",
    "                # tanh_gain = nn.init.calculate_gain('tanh')\n",
    "                # a *= tanh_gain\n",
    "                #\n",
    "                # nn.init.uniform_(m.weight.data, -a, a)\n",
    "\n",
    "                # nn.init.xavier_uniform_(m.weight.data, gain=tanh_gain)\n",
    "\n",
    "                # nn.init.normal_(m.weight.data, std=np.sqrt(2 / self.neural_num))\n",
    "#                 nn.init.kaiming_normal_(m.weight.data)\n",
    "\n",
    "# 100层神经网络\n",
    "layer_nums = 100\n",
    "# 每一层神经元数为256\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:使用Xavier初始化tanh激活函数的全连接网络解决梯度消失问题的代码示例？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.7571136355400085\n",
      "layer:1, std:0.6924336552619934\n",
      "layer:2, std:0.6677976846694946\n",
      "layer:3, std:0.6551960110664368\n",
      "layer:4, std:0.655646800994873\n",
      "layer:5, std:0.6536089777946472\n",
      "layer:6, std:0.6500504612922668\n",
      "layer:7, std:0.6465446949005127\n",
      "layer:8, std:0.6456685662269592\n",
      "layer:9, std:0.6414617896080017\n",
      "layer:10, std:0.6423627734184265\n",
      "layer:11, std:0.6509683728218079\n",
      "layer:12, std:0.6584846377372742\n",
      "layer:13, std:0.6530249118804932\n",
      "layer:14, std:0.6528729796409607\n",
      "layer:15, std:0.6523412466049194\n",
      "layer:16, std:0.6534921526908875\n",
      "layer:17, std:0.6540238261222839\n",
      "layer:18, std:0.6477403044700623\n",
      "layer:19, std:0.6469652652740479\n",
      "layer:20, std:0.6441705822944641\n",
      "layer:21, std:0.6484488248825073\n",
      "layer:22, std:0.6512865424156189\n",
      "layer:23, std:0.6525684595108032\n",
      "layer:24, std:0.6531476378440857\n",
      "layer:25, std:0.6488809585571289\n",
      "layer:26, std:0.6533839702606201\n",
      "layer:27, std:0.6482065320014954\n",
      "layer:28, std:0.6471589803695679\n",
      "layer:29, std:0.6553042531013489\n",
      "layer:30, std:0.6560811400413513\n",
      "layer:31, std:0.6522760987281799\n",
      "layer:32, std:0.6499098539352417\n",
      "layer:33, std:0.6568747758865356\n",
      "layer:34, std:0.6544532179832458\n",
      "layer:35, std:0.6535674929618835\n",
      "layer:36, std:0.6508696675300598\n",
      "layer:37, std:0.6428772807121277\n",
      "layer:38, std:0.6495102643966675\n",
      "layer:39, std:0.6479291319847107\n",
      "layer:40, std:0.6470604538917542\n",
      "layer:41, std:0.6513484716415405\n",
      "layer:42, std:0.6503545045852661\n",
      "layer:43, std:0.6458993554115295\n",
      "layer:44, std:0.6517387628555298\n",
      "layer:45, std:0.6520006060600281\n",
      "layer:46, std:0.6539937257766724\n",
      "layer:47, std:0.6537032723426819\n",
      "layer:48, std:0.6516646146774292\n",
      "layer:49, std:0.6535552740097046\n",
      "layer:50, std:0.6464877724647522\n",
      "layer:51, std:0.6491119265556335\n",
      "layer:52, std:0.6455202102661133\n",
      "layer:53, std:0.6520237326622009\n",
      "layer:54, std:0.6531855463981628\n",
      "layer:55, std:0.6627183556556702\n",
      "layer:56, std:0.6544181108474731\n",
      "layer:57, std:0.6501768827438354\n",
      "layer:58, std:0.6510448455810547\n",
      "layer:59, std:0.6549468040466309\n",
      "layer:60, std:0.6529951691627502\n",
      "layer:61, std:0.6515748500823975\n",
      "layer:62, std:0.6453633904457092\n",
      "layer:63, std:0.644793689250946\n",
      "layer:64, std:0.6489539742469788\n",
      "layer:65, std:0.6553947925567627\n",
      "layer:66, std:0.6535270810127258\n",
      "layer:67, std:0.6528791785240173\n",
      "layer:68, std:0.6492816209793091\n",
      "layer:69, std:0.6596571207046509\n",
      "layer:70, std:0.6536712646484375\n",
      "layer:71, std:0.6498764157295227\n",
      "layer:72, std:0.6538681387901306\n",
      "layer:73, std:0.64595627784729\n",
      "layer:74, std:0.6543275117874146\n",
      "layer:75, std:0.6525828838348389\n",
      "layer:76, std:0.6462088227272034\n",
      "layer:77, std:0.6534948945045471\n",
      "layer:78, std:0.6461930871009827\n",
      "layer:79, std:0.6457878947257996\n",
      "layer:80, std:0.6481245160102844\n",
      "layer:81, std:0.6496317386627197\n",
      "layer:82, std:0.6516988277435303\n",
      "layer:83, std:0.6485154032707214\n",
      "layer:84, std:0.6395408511161804\n",
      "layer:85, std:0.6498249173164368\n",
      "layer:86, std:0.6510564088821411\n",
      "layer:87, std:0.6505221724510193\n",
      "layer:88, std:0.6573457717895508\n",
      "layer:89, std:0.6529723405838013\n",
      "layer:90, std:0.6536353230476379\n",
      "layer:91, std:0.6497699022293091\n",
      "layer:92, std:0.6459059715270996\n",
      "layer:93, std:0.6459072232246399\n",
      "layer:94, std:0.6530925631523132\n",
      "layer:95, std:0.6515892148017883\n",
      "layer:96, std:0.6434286832809448\n",
      "layer:97, std:0.6425578594207764\n",
      "layer:98, std:0.6407340168952942\n",
      "layer:99, std:0.6442393660545349\n",
      "tensor([[ 0.1133,  0.1239,  0.8211,  ...,  0.9411, -0.6334,  0.5155],\n",
      "        [-0.9585, -0.2371,  0.8548,  ..., -0.2339,  0.9326,  0.0114],\n",
      "        [ 0.9487, -0.2279,  0.8735,  ..., -0.9593,  0.7922,  0.6263],\n",
      "        ...,\n",
      "        [ 0.7257,  0.0800, -0.4440,  ..., -0.9589,  0.2604,  0.5402],\n",
      "        [-0.9572,  0.5179, -0.8041,  ..., -0.4298, -0.6087,  0.9679],\n",
      "        [ 0.6105,  0.3994,  0.1072,  ...,  0.3904, -0.5274,  0.0776]],\n",
      "       grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from utils.common_tools import set_seed\n",
    "\n",
    "set_seed(1)  # 设置随机种子\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            # 多添加了tanh激活函数\n",
    "            x = torch.tanh(x)\n",
    "\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # 手动计算xavier\n",
    "                a = np.sqrt(6 / (self.neural_num + self.neural_num))\n",
    "                tanh_gain = nn.init.calculate_gain('tanh')\n",
    "                a *= tanh_gain\n",
    "                nn.init.uniform_(m.weight.data, -a, a)\n",
    "                # pytorch提供的xavier初始化方法，效果同上\n",
    "                # tanh_gain = nn.init.calculate_gain('tanh')\n",
    "                # nn.init.xavier_uniform_(m.weight.data, gain=tanh_gain)\n",
    "\n",
    "                # nn.init.normal_(m.weight.data, std=np.sqrt(2 / self.neural_num))\n",
    "#                 nn.init.kaiming_normal_(m.weight.data)\n",
    "\n",
    "# 100层神经网络\n",
    "layer_nums = 100\n",
    "# 每一层神经元数为256\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q:使用He初始化(Kaiming)relu激活函数的全连接网络解决梯度消失问题的代码示例？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.826629638671875\n",
      "layer:1, std:0.8786815404891968\n",
      "layer:2, std:0.9134422540664673\n",
      "layer:3, std:0.8892471194267273\n",
      "layer:4, std:0.834428071975708\n",
      "layer:5, std:0.874537467956543\n",
      "layer:6, std:0.7926971316337585\n",
      "layer:7, std:0.7806458473205566\n",
      "layer:8, std:0.8684563636779785\n",
      "layer:9, std:0.9434137344360352\n",
      "layer:10, std:0.964215874671936\n",
      "layer:11, std:0.8896796107292175\n",
      "layer:12, std:0.8287257552146912\n",
      "layer:13, std:0.8519769906997681\n",
      "layer:14, std:0.8354345560073853\n",
      "layer:15, std:0.802306056022644\n",
      "layer:16, std:0.8613607287406921\n",
      "layer:17, std:0.7583686709403992\n",
      "layer:18, std:0.8120225071907043\n",
      "layer:19, std:0.791111171245575\n",
      "layer:20, std:0.7164372801780701\n",
      "layer:21, std:0.778393030166626\n",
      "layer:22, std:0.8672043085098267\n",
      "layer:23, std:0.874812662601471\n",
      "layer:24, std:0.9020991325378418\n",
      "layer:25, std:0.8585715889930725\n",
      "layer:26, std:0.7824353575706482\n",
      "layer:27, std:0.7968912720680237\n",
      "layer:28, std:0.8984369039535522\n",
      "layer:29, std:0.8704465627670288\n",
      "layer:30, std:0.9860473275184631\n",
      "layer:31, std:0.9080777168273926\n",
      "layer:32, std:0.9140636920928955\n",
      "layer:33, std:1.009956955909729\n",
      "layer:34, std:0.9909380674362183\n",
      "layer:35, std:1.0253208875656128\n",
      "layer:36, std:0.849043607711792\n",
      "layer:37, std:0.703953742980957\n",
      "layer:38, std:0.7186155319213867\n",
      "layer:39, std:0.7250635027885437\n",
      "layer:40, std:0.7030817270278931\n",
      "layer:41, std:0.6325559020042419\n",
      "layer:42, std:0.6623690724372864\n",
      "layer:43, std:0.6960875988006592\n",
      "layer:44, std:0.7140733003616333\n",
      "layer:45, std:0.632905125617981\n",
      "layer:46, std:0.6458898186683655\n",
      "layer:47, std:0.7354375720024109\n",
      "layer:48, std:0.6710687279701233\n",
      "layer:49, std:0.6939153671264648\n",
      "layer:50, std:0.6889258027076721\n",
      "layer:51, std:0.6331773996353149\n",
      "layer:52, std:0.6029313206672668\n",
      "layer:53, std:0.6145528554916382\n",
      "layer:54, std:0.6636686325073242\n",
      "layer:55, std:0.7440094947814941\n",
      "layer:56, std:0.7972175478935242\n",
      "layer:57, std:0.7606149911880493\n",
      "layer:58, std:0.696868360042572\n",
      "layer:59, std:0.7306802272796631\n",
      "layer:60, std:0.6875627636909485\n",
      "layer:61, std:0.7171440720558167\n",
      "layer:62, std:0.7646605372428894\n",
      "layer:63, std:0.7965086698532104\n",
      "layer:64, std:0.8833740949630737\n",
      "layer:65, std:0.8592952489852905\n",
      "layer:66, std:0.8092936873435974\n",
      "layer:67, std:0.806481122970581\n",
      "layer:68, std:0.6792410612106323\n",
      "layer:69, std:0.6583346128463745\n",
      "layer:70, std:0.5702278017997742\n",
      "layer:71, std:0.5084435939788818\n",
      "layer:72, std:0.4869326055049896\n",
      "layer:73, std:0.46350404620170593\n",
      "layer:74, std:0.4796811640262604\n",
      "layer:75, std:0.47372108697891235\n",
      "layer:76, std:0.45414549112319946\n",
      "layer:77, std:0.4971912205219269\n",
      "layer:78, std:0.492794930934906\n",
      "layer:79, std:0.4422350823879242\n",
      "layer:80, std:0.4802998900413513\n",
      "layer:81, std:0.5579248666763306\n",
      "layer:82, std:0.5283755660057068\n",
      "layer:83, std:0.5451980829238892\n",
      "layer:84, std:0.6203726530075073\n",
      "layer:85, std:0.6571893095970154\n",
      "layer:86, std:0.703682005405426\n",
      "layer:87, std:0.7321067452430725\n",
      "layer:88, std:0.6924356818199158\n",
      "layer:89, std:0.6652532815933228\n",
      "layer:90, std:0.6728308796882629\n",
      "layer:91, std:0.6606621742248535\n",
      "layer:92, std:0.6094604730606079\n",
      "layer:93, std:0.6019102334976196\n",
      "layer:94, std:0.595421552658081\n",
      "layer:95, std:0.6624555587768555\n",
      "layer:96, std:0.6377885341644287\n",
      "layer:97, std:0.6079285740852356\n",
      "layer:98, std:0.6579315066337585\n",
      "layer:99, std:0.6668476462364197\n",
      "tensor([[0.0000, 1.3437, 0.0000,  ..., 0.0000, 0.6444, 1.1867],\n",
      "        [0.0000, 0.9757, 0.0000,  ..., 0.0000, 0.4645, 0.8594],\n",
      "        [0.0000, 1.0023, 0.0000,  ..., 0.0000, 0.5148, 0.9196],\n",
      "        ...,\n",
      "        [0.0000, 1.2873, 0.0000,  ..., 0.0000, 0.6454, 1.1411],\n",
      "        [0.0000, 1.3589, 0.0000,  ..., 0.0000, 0.6749, 1.2438],\n",
      "        [0.0000, 1.1807, 0.0000,  ..., 0.0000, 0.5668, 1.0600]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from utils.common_tools import set_seed\n",
    "\n",
    "set_seed(1)  # 设置随机种子\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            # 多添加了relu激活函数\n",
    "            x = torch.relu(x)\n",
    "\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # 手动计算kaiming初始化\n",
    "                nn.init.normal_(m.weight.data, std=np.sqrt(2 / self.neural_num))\n",
    "                # pytorch官方提供kaiming初始化\n",
    "                # nn.init.kaiming_normal_(m.weight.data)\n",
    "\n",
    "# 100层神经网络\n",
    "layer_nums = 100\n",
    "# 每一层神经元数为256\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:如何计算激活函数的方差变化尺度？\n",
    "- `torch.nn.init.calculate_gain(nonlinearity, param=None)`\n",
    "- nonlinearity：激活函数名称\n",
    "- param：激活函数的参数，如Leaky ReLU的negative_slop\n",
    "\n",
    "Q:calculate_gain代码示例？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gain:1.5827221870422363\n",
      "tanh_gain in PyTorch: 1.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 数据\n",
    "x = torch.randn(10000)\n",
    "out = torch.tanh(x)\n",
    "\n",
    "# 得到标准差的尺度变化，即缩放比例\n",
    "gain = x.std() / out.std()\n",
    "print('gain:{}'.format(gain))\n",
    "\n",
    "# pytorch计算tanh的增益\n",
    "tanh_gain = nn.init.calculate_gain('tanh')\n",
    "print('tanh_gain in PyTorch:', tanh_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.损失函数（一）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
